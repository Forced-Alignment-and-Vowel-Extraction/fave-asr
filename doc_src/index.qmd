---
title: FAVE Automated Speech Recognition
---

![PyPI](https://img.shields.io/pypi/v/fave-asr)
![Build status](https://github.com/Forced-Alignment-and-Vowel-Extraction/fave-asr/actions/workflows/build.yml/badge.svg)
[![codecov](https://codecov.io/gh/Forced-Alignment-and-Vowel-Extraction/fave-asr/graph/badge.svg?token=V54YXTIOPQ)](https://codecov.io/gh/Forced-Alignment-and-Vowel-Extraction/fave-asr)
[![Build Docs](https://github.com/Forced-Alignment-and-Vowel-Extraction/fave-asr/actions/workflows/quarto_docs.yml/badge.svg)](https://forced-alignment-and-vowel-extraction.github.io/fave-asr/)

The FAVE-asr package provides a system for the automated transcription of sociolinguistic interview data on local machines for use by aligners like [FAVE](https://github.com/JoFrhwld/FAVE) or the [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/latest/). The package provides functions to label different speakers in the same audio (diarization), transcribe speech, and output TextGrids with phrase- or word-level alignments.

## Example Use Cases

- You want a transcription of an interview for more detailed hand correction.
- You want to transcribe a large corpus and your analysis can tolerate a small error rate.
- You want to make an audio corpus into a text corpus.
- You want to know the number of speakers in an audio file.

For examples on how to use the pacakge, see the [Usage](usage/) pages.

## Installation
To install fave-asr using pip, run the following command in your terminal:

```bash
pip install fave-asr
```


## Not another transcription service

There are several services which automate the process of transcribing audio, including

- [DARLA CAVE](http://darla.dartmouth.edu/cave)
- [CLOx](https://clox.ling.washington.edu/#/)
- [Otter AI](https://otter.ai/)

Unlike other services, `fave-asr` does not require uploading your data to other servers and instead focuses on processing audio on your own computer. Audio data can contain highly confidential information, and uploading this data to other services may not comply with ethical or legal data protection obligations. The goal of `fave-asr` is to serve those use cases where data protection makes local transcription necessary while making the process as seamless as cloud-based transcription services. 

### Example

As an example, we'll transcribe an audio interview of Snoop Dogg by the 85 South Media podcast and output it as a TextGrid.

```{python}
#| output: false
import os
import fave_asr

data = fave_asr.transcribe_and_diarize(
    audio_file = 'usage/resources/SnoopDogg_85SouthMedia.wav',
    hf_token = os.environ["HF_TOKEN"],
    model_name = 'small.en',
    device = 'cpu'
    )
tg = fave_asr.to_TextGrid(data)
tg.write('SnoopDogg_85SouthMedia.TextGrid')
```
```{python}
#| echo: false
import io
buffer = io.StringIO()
# textgrid.write() closes the buffer, preventing us from reading it, so trick it
# into calling nothing
close = buffer.close
buffer.close = lambda: None
tg.write(buffer)
print(buffer.getvalue())
```

## For more

- To start jumping in, check out [the quickstart](usage/index.qmd)
- To learn how to set up and use the gated models, check out [the gated model documentation](usage/gated_models.qmd)

You can also directly read up on [the function and class references](reference/index.qmd).

## Authors
The speaker diarization pipeline is based on [an article and code by Lu√≠s Roque](https://github.com/luisroque/large_laguage_models/blob/062d3c1d77da3bafa8f52a951eac099480ce3b15/speech2text_whisperai_pyannotate.py) released under [the CC-By-4.0 license](https://creativecommons.org/licenses/by/4.0/). Christian Brickhouse modified that work to use the whisper-timestamped model and for use as a library. For licensing of the test audio, see the README in that directory.

### Recommended citation
Brickhouse, Christian (2024). FAVE-ASR: Offline transcription of interview data (Version 0.1.0) [computer software]. https://forced-alignment-and-vowel-extraction.github.io/fave-asr/
