[
  {
    "objectID": "reference/fave_asr.html",
    "href": "reference/fave_asr.html",
    "title": "fave_asr",
    "section": "",
    "text": "fave_asr\nThis module automates the transcription and diarization of linguistic data.\n\n\n\n\n\nName\nDescription\n\n\n\n\nalign_segments\nAlign the transcript segments using a pretrained alignment model (Wav2Vec2 by default).\n\n\nassign_speakers\nAssign speakers to each transcript segment based on the speaker diarization result.\n\n\ndiarize\nPerform speaker diarization on an audio file.\n\n\nto_TextGrid\nConvert a diarized transcription dictionary to a TextGrid\n\n\ntranscribe\nTranscribe an audio file using a whisper model.\n\n\ntranscribe_and_diarize\nTranscribe an audio file and perform speaker diarization.\n\n\n\n\n\nfave_asr.align_segments(segments, language_code, audio_file, device='cpu')\nAlign the transcript segments using a pretrained alignment model (Wav2Vec2 by default).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsegments\ntyping.List[typing.Dict[str, typing.Any]]\nList of transcript segments to align.\nrequired\n\n\nlanguage_code\nstr\nLanguage code of the audio file.\nrequired\n\n\naudio_file\nstr\nPath to the audio file containing the audio data.\nrequired\n\n\ndevice\nstr\nThe device to use for inference (e.g., “cpu” or “cuda”).\n'cpu'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Dict[str, typing.Any]\nA dictionary representing the aligned transcript segments.\n\n\n\n\n\n\n\nfave_asr.assign_speakers(diarization_result, aligned_segments)\nAssign speakers to each transcript segment based on the speaker diarization result.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndiarization_result\ntyping.Dict[str, typing.Any]\nDictionary of diarized audio file, including speaker embeddings and number of speakers.\nrequired\n\n\naligned_segments\ntyping.Dict[str, typing.Any]\nDictionary representing the aligned transcript segments.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.List[typing.Dict[str, typing.Any]]\nA list of dictionaries representing each segment of the transcript, including\n\n\ntyping.List[typing.Dict[str, typing.Any]]\nthe start and end times, the spoken text, and the speaker ID.\n\n\n\n\n\n\n\nfave_asr.diarize(audio_file, hf_token)\nPerform speaker diarization on an audio file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naudio_file\nstr\nPath to the audio file to diarize.\nrequired\n\n\nhf_token\nstr\nAuthentication token for accessing the Hugging Face API.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Dict[str, typing.Any]\nA dictionary representing the diarized audio file, including the speaker embeddings and the number of speakers.\n\n\n\n\n\n\n\nfave_asr.to_TextGrid(diarized_transcription, by_phrase=True)\nConvert a diarized transcription dictionary to a TextGrid\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndiarized_transcription\n\nOutput of pipeline.assign_speakers()\nrequired\n\n\nby_phrase\n\nFlag for whether the intervals should be by phrase (True) or word (False)\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n\nA textgrid.TextGrid object populated with the diarized and\n\n\n\ntranscribed data. Tiers are by speaker and contain word-level\n\n\n\nintervals not utterance-level.\n\n\n\n\n\n\n\nfave_asr.transcribe(audio_file, model_name, device='cpu', detect_disfluencies=True)\nTranscribe an audio file using a whisper model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naudio_file\nstr\nPath to the audio file to transcribe.\nrequired\n\n\nmodel_name\nstr\nName of the model to use for transcription.\nrequired\n\n\ndevice\nstr\nThe device to use for inference (e.g., “cpu” or “cuda”).\n'cpu'\n\n\ndetect_disfluencies\nbool\nFlag for whether the transcription should include disfluencies, marked with [*]\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Dict[str, typing.Any]\nA dictionary representing the transcript segments and language code.\n\n\n\n\n\n\n\nfave_asr.transcribe_and_diarize(audio_file, hf_token, model_name, device='cpu')\nTranscribe an audio file and perform speaker diarization.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naudio_file\nstr\nPath to the audio file to transcribe and diarize.\nrequired\n\n\nhf_token\nstr\nAuthentication token for accessing the Hugging Face API.\nrequired\n\n\nmodel_name\nstr\nName of the model to use for transcription.\nrequired\n\n\ndevice\nstr\nThe device to use for inference (e.g., “cpu” or “cuda”).\n'cpu'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.List[typing.Dict[str, typing.Any]]\nA list of dictionaries representing each segment of the transcript, including\n\n\ntyping.List[typing.Dict[str, typing.Any]]\nthe start and end times, the spoken text, and the speaker ID.",
    "crumbs": [
      "FAVE ASR functions",
      "fave_asr"
    ]
  },
  {
    "objectID": "reference/fave_asr.html#functions",
    "href": "reference/fave_asr.html#functions",
    "title": "fave_asr",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nalign_segments\nAlign the transcript segments using a pretrained alignment model (Wav2Vec2 by default).\n\n\nassign_speakers\nAssign speakers to each transcript segment based on the speaker diarization result.\n\n\ndiarize\nPerform speaker diarization on an audio file.\n\n\nto_TextGrid\nConvert a diarized transcription dictionary to a TextGrid\n\n\ntranscribe\nTranscribe an audio file using a whisper model.\n\n\ntranscribe_and_diarize\nTranscribe an audio file and perform speaker diarization.\n\n\n\n\n\nfave_asr.align_segments(segments, language_code, audio_file, device='cpu')\nAlign the transcript segments using a pretrained alignment model (Wav2Vec2 by default).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsegments\ntyping.List[typing.Dict[str, typing.Any]]\nList of transcript segments to align.\nrequired\n\n\nlanguage_code\nstr\nLanguage code of the audio file.\nrequired\n\n\naudio_file\nstr\nPath to the audio file containing the audio data.\nrequired\n\n\ndevice\nstr\nThe device to use for inference (e.g., “cpu” or “cuda”).\n'cpu'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Dict[str, typing.Any]\nA dictionary representing the aligned transcript segments.\n\n\n\n\n\n\n\nfave_asr.assign_speakers(diarization_result, aligned_segments)\nAssign speakers to each transcript segment based on the speaker diarization result.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndiarization_result\ntyping.Dict[str, typing.Any]\nDictionary of diarized audio file, including speaker embeddings and number of speakers.\nrequired\n\n\naligned_segments\ntyping.Dict[str, typing.Any]\nDictionary representing the aligned transcript segments.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.List[typing.Dict[str, typing.Any]]\nA list of dictionaries representing each segment of the transcript, including\n\n\ntyping.List[typing.Dict[str, typing.Any]]\nthe start and end times, the spoken text, and the speaker ID.\n\n\n\n\n\n\n\nfave_asr.diarize(audio_file, hf_token)\nPerform speaker diarization on an audio file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naudio_file\nstr\nPath to the audio file to diarize.\nrequired\n\n\nhf_token\nstr\nAuthentication token for accessing the Hugging Face API.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Dict[str, typing.Any]\nA dictionary representing the diarized audio file, including the speaker embeddings and the number of speakers.\n\n\n\n\n\n\n\nfave_asr.to_TextGrid(diarized_transcription, by_phrase=True)\nConvert a diarized transcription dictionary to a TextGrid\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndiarized_transcription\n\nOutput of pipeline.assign_speakers()\nrequired\n\n\nby_phrase\n\nFlag for whether the intervals should be by phrase (True) or word (False)\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n\nA textgrid.TextGrid object populated with the diarized and\n\n\n\ntranscribed data. Tiers are by speaker and contain word-level\n\n\n\nintervals not utterance-level.\n\n\n\n\n\n\n\nfave_asr.transcribe(audio_file, model_name, device='cpu', detect_disfluencies=True)\nTranscribe an audio file using a whisper model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naudio_file\nstr\nPath to the audio file to transcribe.\nrequired\n\n\nmodel_name\nstr\nName of the model to use for transcription.\nrequired\n\n\ndevice\nstr\nThe device to use for inference (e.g., “cpu” or “cuda”).\n'cpu'\n\n\ndetect_disfluencies\nbool\nFlag for whether the transcription should include disfluencies, marked with [*]\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Dict[str, typing.Any]\nA dictionary representing the transcript segments and language code.\n\n\n\n\n\n\n\nfave_asr.transcribe_and_diarize(audio_file, hf_token, model_name, device='cpu')\nTranscribe an audio file and perform speaker diarization.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naudio_file\nstr\nPath to the audio file to transcribe and diarize.\nrequired\n\n\nhf_token\nstr\nAuthentication token for accessing the Hugging Face API.\nrequired\n\n\nmodel_name\nstr\nName of the model to use for transcription.\nrequired\n\n\ndevice\nstr\nThe device to use for inference (e.g., “cpu” or “cuda”).\n'cpu'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.List[typing.Dict[str, typing.Any]]\nA list of dictionaries representing each segment of the transcript, including\n\n\ntyping.List[typing.Dict[str, typing.Any]]\nthe start and end times, the spoken text, and the speaker ID.",
    "crumbs": [
      "FAVE ASR functions",
      "fave_asr"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FAVE Automated Speech Recognition",
    "section": "",
    "text": "The FAVE-asr package provides a system for the automated transcription of sociolinguistic interview data on local machines for use by aligners like FAVE or the Montreal Forced Aligner. The package provides functions to label different speakers in the same audio (diarization), transcribe speech, and output TextGrids with phrase- or word-level alignments.",
    "crumbs": [
      "Get Started",
      "Getting Started",
      "FAVE Automated Speech Recognition"
    ]
  },
  {
    "objectID": "index.html#example-use-cases",
    "href": "index.html#example-use-cases",
    "title": "FAVE Automated Speech Recognition",
    "section": "Example Use Cases",
    "text": "Example Use Cases\n\nYou want a transcription of an interview for more detailed hand correction.\nYou want to transcribe a large corpus and your analysis can tolerate a small error rate.\nYou want to make an audio corpus into a text corpus.\nYou want to know the number of speakers in an audio file.\n\nFor examples on how to use the pacakge, see the Usage pages.",
    "crumbs": [
      "Get Started",
      "Getting Started",
      "FAVE Automated Speech Recognition"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "FAVE Automated Speech Recognition",
    "section": "Installation",
    "text": "Installation\nTo install fave-asr using pip, run the following command in your terminal:\npip install fave-asr",
    "crumbs": [
      "Get Started",
      "Getting Started",
      "FAVE Automated Speech Recognition"
    ]
  },
  {
    "objectID": "index.html#not-another-transcription-service",
    "href": "index.html#not-another-transcription-service",
    "title": "FAVE Automated Speech Recognition",
    "section": "Not another transcription service",
    "text": "Not another transcription service\nThere are several services which automate the process of transcribing audio, including\n\nDARLA CAVE\nOtter AI\n\nUnlike other services, fave-asr does not require uploading your data to other servers and instead focuses on processing audio on your own computer. Audio data can contain highly confidential information, and uploading this data to other services may not comply with ethical or legal data protection obligations. The goal of fave-asr is to serve those use cases where data protection makes local transcription necessary while making the process as seamless as cloud-based transcription services.\n\nExample\nAs an example, we’ll transcribe an audio interview of Snoop Dogg by the 85 South Media podcast and output it as a TextGrid.\n\nimport os\nimport fave_asr\n\ndata = fave_asr.transcribe_and_diarize(\n    audio_file = 'usage/resources/SnoopDogg_85SouthMedia.wav',\n    hf_token = os.environ[\"HF_TOKEN\"],\n    model_name = 'small.en',\n    device = 'cpu'\n    )\ntg = fave_asr.to_TextGrid(data)\ntg.write('SnoopDogg_85SouthMedia.TextGrid')\n\n\n\nFile type = \"ooTextFile\"\nObject class = \"TextGrid\"\n\nxmin = 0.0\nxmax = 51.44\ntiers? &lt;exists&gt;\nsize = 1\nitem []:\n    item [1]:\n        class = \"IntervalTier\"\n        name = \"SPEAKER_00\"\n        xmin = 0.0\n        xmax = 51.44\n        intervals: size = 12\n            intervals [1]:\n                xmin = 0.0\n                xmax = 6.24\n                text = \" So you know the pimpin fuck y'all I'm gonna go over to dev jam\"\n            intervals [2]:\n                xmin = 6.24\n                xmax = 8.78\n                text = \" And learn a little bit of corporate work cuz I don't know corporate shit\"\n            intervals [3]:\n                xmin = 8.78\n                xmax = 11.46\n                text = \" I only need a few months right give me a few months around the shit\"\n            intervals [4]:\n                xmin = 11.46\n                xmax = 18.94\n                text = \" I'm a fast learner go to dev jam get a job in a position drop a record get Benny the butcher song get hip-hop Harry's on\"\n            intervals [5]:\n                xmin = 18.94\n                xmax = 23.94\n                text = \" Learn a few tricks of the trade find out that the niggas that had it that wanted me to hold for them\"\n            intervals [6]:\n                xmin = 23.94\n                xmax = 26.12\n                text = \" Then sold it to some other people\"\n            intervals [7]:\n                xmin = 26.12\n                xmax = 29.9\n                text = \" So now one of my big wig buddies called me and say hey dog\"\n            intervals [8]:\n                xmin = 29.9\n                xmax = 34.38\n                text = \" I know the people that got there from and they don't know what to do with it. Mmm\"\n            intervals [9]:\n                xmin = 34.38\n                xmax = 40.72\n                text = \" Let me hide them. I know just what to do with it. So I hit them like let me um, we work for y'all\"\n            intervals [10]:\n                xmin = 40.72\n                xmax = 45.96\n                text = \" The play was cool, but it's like yeah fuck that how much how much to buy this shit?\"\n            intervals [11]:\n                xmin = 45.96\n                xmax = 50.22\n                text = \" How much to buy death row first how much for my masters?\"\n            intervals [12]:\n                xmin = 50.22\n                xmax = 51.44\n                text = \" How much for all of the masters?\"",
    "crumbs": [
      "Get Started",
      "Getting Started",
      "FAVE Automated Speech Recognition"
    ]
  },
  {
    "objectID": "index.html#for-more",
    "href": "index.html#for-more",
    "title": "FAVE Automated Speech Recognition",
    "section": "For more",
    "text": "For more\n\nTo start jumping in, check out the quickstart\nTo learn how to set up and use the gated models, check out the gated model documentation\n\nYou can also directly read up on the function and class references.",
    "crumbs": [
      "Get Started",
      "Getting Started",
      "FAVE Automated Speech Recognition"
    ]
  },
  {
    "objectID": "usage/index.html",
    "href": "usage/index.html",
    "title": "Usage examples",
    "section": "",
    "text": "The fave-asr pipeline automates a few different steps that can be broken down depending on your needs. For example, if you just need a transcript but don’t care about who said the words, you can just do the transcribe step and none of the others.\n\n\n\nimport fave_asr\n\ntranscription = fave_asr.transcribe(\n    audio_file = 'resources/SnoopDogg_85SouthMedia.wav',\n    model_name = 'small.en',\n    device = 'cpu'\n    )\n\nThe output in transcription is a dictionary with the keys segments and language_code. segments is a List of Dicts, with each Dict having data on the speech in that segment.\n\ntranscription['segments'][0].keys()\n\ndict_keys(['id', 'seek', 'start', 'end', 'text', 'tokens', 'temperature', 'avg_logprob', 'compression_ratio', 'no_speech_prob', 'confidence', 'words'])\n\n\nIf you wanted a text transcript of the entire file, you can iterate through segments and get the text field for each one.\n\ntext_list = []\nfor segment in transcription['segments']:\n    text_list.append(segment['text'])\nprint(\"\\n\".join(text_list))\n\n So you know the pimpin fuck y'all I'm gonna go over to dev jam\n And learn a little bit of corporate work cuz I don't know corporate shit\n I only need a few months right give me a few months around the shit\n I'm a fast learner go to dev jam get a job in a position drop a record get Benny the butcher song get hip-hop Harry's on\n Learn a few tricks of the trade find out that the niggas that had it that wanted me to hold for them\n Then sold it to some other people\n So now one of my big wig buddies called me and say hey dog\n I know the people that got there from and they don't know what to do with it. Mmm\n Let me hide them. I know just what to do with it. So I hit them like let me um, we work for y'all\n The play was cool, but it's like yeah fuck that how much how much to buy this shit?\n How much to buy death row first how much for my masters?\n How much for all of the masters?\n\n\nEach segment also has word-level data available in the words field including start and end times for each word.\n\n\n\n\n\n\n\n\n\nGated model access\n\n\n\nDiarization requires a HuggingFace Access Token and that you agree to the terms of some gated models. See the documentation page on setting and using access tokens\n\n\nSome audio files have more than one speaker, and a raw transcript may not be useful if we don’t know who said what. The process of assigning speech to a speaker in an audio file is diarization. fave-asr uses machine learning models which are gated, meaning that the creators might require you to agree to particular terms before using it. You can learn more and agree to the terms at the page for the diarization model.\n\nimport os\ndiarization = fave_asr.diarize(\n    audio_file = 'resources/SnoopDogg_85SouthMedia.wav',\n    hf_token=os.environ[\"HF_TOKEN\"]\n    )\nprint(diarization)\n\n                              segment label     speaker      start        end\n0   [ 00:00:00.365 --&gt;  00:00:00.704]     A  SPEAKER_00   0.365025   0.704584\n1   [ 00:00:01.044 --&gt;  00:00:01.825]     B  SPEAKER_00   1.044143   1.825127\n2   [ 00:00:02.741 --&gt;  00:00:03.251]     C  SPEAKER_00   2.741935   3.251273\n3   [ 00:00:03.947 --&gt;  00:00:05.305]     D  SPEAKER_00   3.947368   5.305603\n4   [ 00:00:06.273 --&gt;  00:00:08.684]     E  SPEAKER_00   6.273345   8.684211\n5   [ 00:00:08.752 --&gt;  00:00:09.057]     F  SPEAKER_01   8.752122   9.057725\n6   [ 00:00:09.057 --&gt;  00:00:12.436]     G  SPEAKER_00   9.057725  12.436333\n7   [ 00:00:09.074 --&gt;  00:00:09.091]     H  SPEAKER_01   9.074703   9.091681\n8   [ 00:00:13.064 --&gt;  00:00:14.915]     I  SPEAKER_00  13.064516  14.915110\n9   [ 00:00:15.271 --&gt;  00:00:18.276]     J  SPEAKER_00  15.271647  18.276740\n10  [ 00:00:18.955 --&gt;  00:00:21.027]     K  SPEAKER_00  18.955857  21.027165\n11  [ 00:00:21.451 --&gt;  00:00:23.539]     L  SPEAKER_00  21.451613  23.539898\n12  [ 00:00:23.998 --&gt;  00:00:25.305]     M  SPEAKER_00  23.998302  25.305603\n13  [ 00:00:25.611 --&gt;  00:00:25.848]     N  SPEAKER_02  25.611205  25.848896\n14  [ 00:00:26.188 --&gt;  00:00:26.833]     O  SPEAKER_00  26.188455  26.833616\n15  [ 00:00:26.460 --&gt;  00:00:26.731]     P  SPEAKER_02  26.460102  26.731749\n16  [ 00:00:27.071 --&gt;  00:00:29.465]     Q  SPEAKER_00  27.071307  29.465195\n17  [ 00:00:29.923 --&gt;  00:00:31.400]     R  SPEAKER_00  29.923599  31.400679\n18  [ 00:00:32.164 --&gt;  00:00:33.098]     S  SPEAKER_00  32.164686  33.098472\n19  [ 00:00:33.488 --&gt;  00:00:33.845]     T  SPEAKER_03  33.488964  33.845501\n20  [ 00:00:34.219 --&gt;  00:00:35.288]     U  SPEAKER_03  34.219015  35.288625\n21  [ 00:00:34.286 --&gt;  00:00:37.224]     V  SPEAKER_00  34.286927  37.224109\n22  [ 00:00:37.427 --&gt;  00:00:38.276]     W  SPEAKER_00  37.427844  38.276740\n23  [ 00:00:38.853 --&gt;  00:00:39.770]     X  SPEAKER_00  38.853990  39.770798\n24  [ 00:00:40.840 --&gt;  00:00:42.707]     Y  SPEAKER_00  40.840407  42.707980\n25  [ 00:00:42.843 --&gt;  00:00:45.152]     Z  SPEAKER_00  42.843803  45.152801\n26  [ 00:00:46.052 --&gt;  00:00:47.461]    AA  SPEAKER_00  46.052632  47.461800\n27  [ 00:00:48.225 --&gt;  00:00:49.329]    AB  SPEAKER_00  48.225806  49.329372\n28  [ 00:00:50.263 --&gt;  00:00:51.553]    AC  SPEAKER_00  50.263158  51.553480\n\n\nThe diarization output is a Pandas DataFrame with various columns. Most important are speaker, start, and end which give a speaker label for that segment, the start time of the segment, and the end time of the segment.\nFor example, you can get a list of unique speaker labels using python’s set function.\n\nspeakers = set(diarization['speaker'])\n\nAnd you can use the len function to get the number of speakers\n\nlen(speakers)\n\n4\n\n\nYou can also filter the transcript by selecting only segments with a particular speaker using Pandas’ DataFrame.loc method.\n\nsnoop_dogg = diarization.loc[diarization['speaker'] == 'SPEAKER_00']\nprint(snoop_dogg)\n\n                              segment label     speaker      start        end\n0   [ 00:00:00.365 --&gt;  00:00:00.704]     A  SPEAKER_00   0.365025   0.704584\n1   [ 00:00:01.044 --&gt;  00:00:01.825]     B  SPEAKER_00   1.044143   1.825127\n2   [ 00:00:02.741 --&gt;  00:00:03.251]     C  SPEAKER_00   2.741935   3.251273\n3   [ 00:00:03.947 --&gt;  00:00:05.305]     D  SPEAKER_00   3.947368   5.305603\n4   [ 00:00:06.273 --&gt;  00:00:08.684]     E  SPEAKER_00   6.273345   8.684211\n6   [ 00:00:09.057 --&gt;  00:00:12.436]     G  SPEAKER_00   9.057725  12.436333\n8   [ 00:00:13.064 --&gt;  00:00:14.915]     I  SPEAKER_00  13.064516  14.915110\n9   [ 00:00:15.271 --&gt;  00:00:18.276]     J  SPEAKER_00  15.271647  18.276740\n10  [ 00:00:18.955 --&gt;  00:00:21.027]     K  SPEAKER_00  18.955857  21.027165\n11  [ 00:00:21.451 --&gt;  00:00:23.539]     L  SPEAKER_00  21.451613  23.539898\n12  [ 00:00:23.998 --&gt;  00:00:25.305]     M  SPEAKER_00  23.998302  25.305603\n14  [ 00:00:26.188 --&gt;  00:00:26.833]     O  SPEAKER_00  26.188455  26.833616\n16  [ 00:00:27.071 --&gt;  00:00:29.465]     Q  SPEAKER_00  27.071307  29.465195\n17  [ 00:00:29.923 --&gt;  00:00:31.400]     R  SPEAKER_00  29.923599  31.400679\n18  [ 00:00:32.164 --&gt;  00:00:33.098]     S  SPEAKER_00  32.164686  33.098472\n21  [ 00:00:34.286 --&gt;  00:00:37.224]     V  SPEAKER_00  34.286927  37.224109\n22  [ 00:00:37.427 --&gt;  00:00:38.276]     W  SPEAKER_00  37.427844  38.276740\n23  [ 00:00:38.853 --&gt;  00:00:39.770]     X  SPEAKER_00  38.853990  39.770798\n24  [ 00:00:40.840 --&gt;  00:00:42.707]     Y  SPEAKER_00  40.840407  42.707980\n25  [ 00:00:42.843 --&gt;  00:00:45.152]     Z  SPEAKER_00  42.843803  45.152801\n26  [ 00:00:46.052 --&gt;  00:00:47.461]    AA  SPEAKER_00  46.052632  47.461800\n27  [ 00:00:48.225 --&gt;  00:00:49.329]    AB  SPEAKER_00  48.225806  49.329372\n28  [ 00:00:50.263 --&gt;  00:00:51.553]    AC  SPEAKER_00  50.263158  51.553480\n\n\n\n\n\nThe last stage of the pipeline is combining the diarization and the transcription by assigning speakers to segments.\n\ndiarized_transcript = fave_asr.assign_speakers(diarization,transcription)\n\nThe structure of diarized_transcript is very similar to the structure of transcription but the segments and words now have a speaker field.\n\ndiarized_transcript['segments'][0]['speaker']\n\n'SPEAKER_00'",
    "crumbs": [
      "Get Started",
      "Getting Started",
      "Usage examples"
    ]
  },
  {
    "objectID": "usage/index.html#pipeline-walkthrough",
    "href": "usage/index.html#pipeline-walkthrough",
    "title": "Usage examples",
    "section": "",
    "text": "The fave-asr pipeline automates a few different steps that can be broken down depending on your needs. For example, if you just need a transcript but don’t care about who said the words, you can just do the transcribe step and none of the others.\n\n\n\nimport fave_asr\n\ntranscription = fave_asr.transcribe(\n    audio_file = 'resources/SnoopDogg_85SouthMedia.wav',\n    model_name = 'small.en',\n    device = 'cpu'\n    )\n\nThe output in transcription is a dictionary with the keys segments and language_code. segments is a List of Dicts, with each Dict having data on the speech in that segment.\n\ntranscription['segments'][0].keys()\n\ndict_keys(['id', 'seek', 'start', 'end', 'text', 'tokens', 'temperature', 'avg_logprob', 'compression_ratio', 'no_speech_prob', 'confidence', 'words'])\n\n\nIf you wanted a text transcript of the entire file, you can iterate through segments and get the text field for each one.\n\ntext_list = []\nfor segment in transcription['segments']:\n    text_list.append(segment['text'])\nprint(\"\\n\".join(text_list))\n\n So you know the pimpin fuck y'all I'm gonna go over to dev jam\n And learn a little bit of corporate work cuz I don't know corporate shit\n I only need a few months right give me a few months around the shit\n I'm a fast learner go to dev jam get a job in a position drop a record get Benny the butcher song get hip-hop Harry's on\n Learn a few tricks of the trade find out that the niggas that had it that wanted me to hold for them\n Then sold it to some other people\n So now one of my big wig buddies called me and say hey dog\n I know the people that got there from and they don't know what to do with it. Mmm\n Let me hide them. I know just what to do with it. So I hit them like let me um, we work for y'all\n The play was cool, but it's like yeah fuck that how much how much to buy this shit?\n How much to buy death row first how much for my masters?\n How much for all of the masters?\n\n\nEach segment also has word-level data available in the words field including start and end times for each word.\n\n\n\n\n\n\n\n\n\nGated model access\n\n\n\nDiarization requires a HuggingFace Access Token and that you agree to the terms of some gated models. See the documentation page on setting and using access tokens\n\n\nSome audio files have more than one speaker, and a raw transcript may not be useful if we don’t know who said what. The process of assigning speech to a speaker in an audio file is diarization. fave-asr uses machine learning models which are gated, meaning that the creators might require you to agree to particular terms before using it. You can learn more and agree to the terms at the page for the diarization model.\n\nimport os\ndiarization = fave_asr.diarize(\n    audio_file = 'resources/SnoopDogg_85SouthMedia.wav',\n    hf_token=os.environ[\"HF_TOKEN\"]\n    )\nprint(diarization)\n\n                              segment label     speaker      start        end\n0   [ 00:00:00.365 --&gt;  00:00:00.704]     A  SPEAKER_00   0.365025   0.704584\n1   [ 00:00:01.044 --&gt;  00:00:01.825]     B  SPEAKER_00   1.044143   1.825127\n2   [ 00:00:02.741 --&gt;  00:00:03.251]     C  SPEAKER_00   2.741935   3.251273\n3   [ 00:00:03.947 --&gt;  00:00:05.305]     D  SPEAKER_00   3.947368   5.305603\n4   [ 00:00:06.273 --&gt;  00:00:08.684]     E  SPEAKER_00   6.273345   8.684211\n5   [ 00:00:08.752 --&gt;  00:00:09.057]     F  SPEAKER_01   8.752122   9.057725\n6   [ 00:00:09.057 --&gt;  00:00:12.436]     G  SPEAKER_00   9.057725  12.436333\n7   [ 00:00:09.074 --&gt;  00:00:09.091]     H  SPEAKER_01   9.074703   9.091681\n8   [ 00:00:13.064 --&gt;  00:00:14.915]     I  SPEAKER_00  13.064516  14.915110\n9   [ 00:00:15.271 --&gt;  00:00:18.276]     J  SPEAKER_00  15.271647  18.276740\n10  [ 00:00:18.955 --&gt;  00:00:21.027]     K  SPEAKER_00  18.955857  21.027165\n11  [ 00:00:21.451 --&gt;  00:00:23.539]     L  SPEAKER_00  21.451613  23.539898\n12  [ 00:00:23.998 --&gt;  00:00:25.305]     M  SPEAKER_00  23.998302  25.305603\n13  [ 00:00:25.611 --&gt;  00:00:25.848]     N  SPEAKER_02  25.611205  25.848896\n14  [ 00:00:26.188 --&gt;  00:00:26.833]     O  SPEAKER_00  26.188455  26.833616\n15  [ 00:00:26.460 --&gt;  00:00:26.731]     P  SPEAKER_02  26.460102  26.731749\n16  [ 00:00:27.071 --&gt;  00:00:29.465]     Q  SPEAKER_00  27.071307  29.465195\n17  [ 00:00:29.923 --&gt;  00:00:31.400]     R  SPEAKER_00  29.923599  31.400679\n18  [ 00:00:32.164 --&gt;  00:00:33.098]     S  SPEAKER_00  32.164686  33.098472\n19  [ 00:00:33.488 --&gt;  00:00:33.845]     T  SPEAKER_03  33.488964  33.845501\n20  [ 00:00:34.219 --&gt;  00:00:35.288]     U  SPEAKER_03  34.219015  35.288625\n21  [ 00:00:34.286 --&gt;  00:00:37.224]     V  SPEAKER_00  34.286927  37.224109\n22  [ 00:00:37.427 --&gt;  00:00:38.276]     W  SPEAKER_00  37.427844  38.276740\n23  [ 00:00:38.853 --&gt;  00:00:39.770]     X  SPEAKER_00  38.853990  39.770798\n24  [ 00:00:40.840 --&gt;  00:00:42.707]     Y  SPEAKER_00  40.840407  42.707980\n25  [ 00:00:42.843 --&gt;  00:00:45.152]     Z  SPEAKER_00  42.843803  45.152801\n26  [ 00:00:46.052 --&gt;  00:00:47.461]    AA  SPEAKER_00  46.052632  47.461800\n27  [ 00:00:48.225 --&gt;  00:00:49.329]    AB  SPEAKER_00  48.225806  49.329372\n28  [ 00:00:50.263 --&gt;  00:00:51.553]    AC  SPEAKER_00  50.263158  51.553480\n\n\nThe diarization output is a Pandas DataFrame with various columns. Most important are speaker, start, and end which give a speaker label for that segment, the start time of the segment, and the end time of the segment.\nFor example, you can get a list of unique speaker labels using python’s set function.\n\nspeakers = set(diarization['speaker'])\n\nAnd you can use the len function to get the number of speakers\n\nlen(speakers)\n\n4\n\n\nYou can also filter the transcript by selecting only segments with a particular speaker using Pandas’ DataFrame.loc method.\n\nsnoop_dogg = diarization.loc[diarization['speaker'] == 'SPEAKER_00']\nprint(snoop_dogg)\n\n                              segment label     speaker      start        end\n0   [ 00:00:00.365 --&gt;  00:00:00.704]     A  SPEAKER_00   0.365025   0.704584\n1   [ 00:00:01.044 --&gt;  00:00:01.825]     B  SPEAKER_00   1.044143   1.825127\n2   [ 00:00:02.741 --&gt;  00:00:03.251]     C  SPEAKER_00   2.741935   3.251273\n3   [ 00:00:03.947 --&gt;  00:00:05.305]     D  SPEAKER_00   3.947368   5.305603\n4   [ 00:00:06.273 --&gt;  00:00:08.684]     E  SPEAKER_00   6.273345   8.684211\n6   [ 00:00:09.057 --&gt;  00:00:12.436]     G  SPEAKER_00   9.057725  12.436333\n8   [ 00:00:13.064 --&gt;  00:00:14.915]     I  SPEAKER_00  13.064516  14.915110\n9   [ 00:00:15.271 --&gt;  00:00:18.276]     J  SPEAKER_00  15.271647  18.276740\n10  [ 00:00:18.955 --&gt;  00:00:21.027]     K  SPEAKER_00  18.955857  21.027165\n11  [ 00:00:21.451 --&gt;  00:00:23.539]     L  SPEAKER_00  21.451613  23.539898\n12  [ 00:00:23.998 --&gt;  00:00:25.305]     M  SPEAKER_00  23.998302  25.305603\n14  [ 00:00:26.188 --&gt;  00:00:26.833]     O  SPEAKER_00  26.188455  26.833616\n16  [ 00:00:27.071 --&gt;  00:00:29.465]     Q  SPEAKER_00  27.071307  29.465195\n17  [ 00:00:29.923 --&gt;  00:00:31.400]     R  SPEAKER_00  29.923599  31.400679\n18  [ 00:00:32.164 --&gt;  00:00:33.098]     S  SPEAKER_00  32.164686  33.098472\n21  [ 00:00:34.286 --&gt;  00:00:37.224]     V  SPEAKER_00  34.286927  37.224109\n22  [ 00:00:37.427 --&gt;  00:00:38.276]     W  SPEAKER_00  37.427844  38.276740\n23  [ 00:00:38.853 --&gt;  00:00:39.770]     X  SPEAKER_00  38.853990  39.770798\n24  [ 00:00:40.840 --&gt;  00:00:42.707]     Y  SPEAKER_00  40.840407  42.707980\n25  [ 00:00:42.843 --&gt;  00:00:45.152]     Z  SPEAKER_00  42.843803  45.152801\n26  [ 00:00:46.052 --&gt;  00:00:47.461]    AA  SPEAKER_00  46.052632  47.461800\n27  [ 00:00:48.225 --&gt;  00:00:49.329]    AB  SPEAKER_00  48.225806  49.329372\n28  [ 00:00:50.263 --&gt;  00:00:51.553]    AC  SPEAKER_00  50.263158  51.553480\n\n\n\n\n\nThe last stage of the pipeline is combining the diarization and the transcription by assigning speakers to segments.\n\ndiarized_transcript = fave_asr.assign_speakers(diarization,transcription)\n\nThe structure of diarized_transcript is very similar to the structure of transcription but the segments and words now have a speaker field.\n\ndiarized_transcript['segments'][0]['speaker']\n\n'SPEAKER_00'",
    "crumbs": [
      "Get Started",
      "Getting Started",
      "Usage examples"
    ]
  },
  {
    "objectID": "usage/index.html#output",
    "href": "usage/index.html#output",
    "title": "Usage examples",
    "section": "Output",
    "text": "Output\n\nTextGrid\nA diarized transcript can be converted to a textgrid object and navigated using that library.\n\ntg = fave_asr.to_TextGrid(diarized_transcript)\n\nYou can write the output to a file using the textgrid.write method by specifying a file name for the output TextGrid.\n\ntg.write('SnoopDogg_Interview.TextGrid')",
    "crumbs": [
      "Get Started",
      "Getting Started",
      "Usage examples"
    ]
  },
  {
    "objectID": "usage/gated_models.html",
    "href": "usage/gated_models.html",
    "title": "Gated models and access tokens",
    "section": "",
    "text": "Artifical Intelegence models are powerful and in the wrong hands can be dangerous. The models used by fave-asr are cost-free, but you need to accept additional terms of use.\nTo use these models: 1. On HuggingFace, create an account or log in 2. Accept the terms and conditions for the segmentation model 3. Accept the terms and conditions for the diarization model 4. Create an access token or copy your existing token\nKeep track of your token and keep it safe (e.g. don’t accidentally upload it to GitHub). We suggest creating an environment variable for your token so that you don’t need to paste it into your files."
  },
  {
    "objectID": "usage/gated_models.html#using-gated-models",
    "href": "usage/gated_models.html#using-gated-models",
    "title": "Gated models and access tokens",
    "section": "",
    "text": "Artifical Intelegence models are powerful and in the wrong hands can be dangerous. The models used by fave-asr are cost-free, but you need to accept additional terms of use.\nTo use these models: 1. On HuggingFace, create an account or log in 2. Accept the terms and conditions for the segmentation model 3. Accept the terms and conditions for the diarization model 4. Create an access token or copy your existing token\nKeep track of your token and keep it safe (e.g. don’t accidentally upload it to GitHub). We suggest creating an environment variable for your token so that you don’t need to paste it into your files."
  },
  {
    "objectID": "usage/gated_models.html#creating-an-environment-variable-for-your-token",
    "href": "usage/gated_models.html#creating-an-environment-variable-for-your-token",
    "title": "Gated models and access tokens",
    "section": "Creating an environment variable for your token",
    "text": "Creating an environment variable for your token\nStoring your tokens as environment variables is a good way to avoid accidentally leaking them. Instead of typing the token into your code and deleting it before you commit, you can use os.environ[\"HF_TOKEN\"] to access it from Python.\n\n\n\n\n\n\nTip\n\n\n\nUsing environment variables makes your code more readable and easier to maintain. Random strings are hard to debug, and you might forget what the token is for. Using environment variables gives these tokens name like HF_TOKEN which is makes it easier to tell what token is being used and if it’s the right one.\n\n\n\nLinux and Mac\nOn Linux and Mac you can store your token in .bashrc\n\nOpen $HOME/.bashrc in a text editor\nAt the end of that file, add the following HF_TOKEN='&lt;your token&gt;' ; export HF_TOKEN replacing &lt;your token&gt; with your HuggingFace token\nAdd the changes to your current session using source $HOME/.bashrc\n\n\n\nWindows\nOn Windows, use the setx command to create an environment variable.\nsetx HF_TOKEN &lt;your token&gt;\n\n\n\n\n\n\nWarning\n\n\n\nYou need to restart the command line afterwards to make the environment variable available for use. If you try to use the variable in the same window you set the variable, you will run into problems."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "fave_asr\nThis module automates the transcription and diarization of linguistic data.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#fave-asr-functions",
    "href": "reference/index.html#fave-asr-functions",
    "title": "Function reference",
    "section": "",
    "text": "fave_asr\nThis module automates the transcription and diarization of linguistic data.",
    "crumbs": [
      "Function reference"
    ]
  }
]